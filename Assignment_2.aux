\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\providecommand\tcolorbox@label[2]{}
\@writefile{toc}{\contentsline {section}{\numberline {1}: (10 points) Exploratory Analysis}{2}{section.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(5 points) Focusing first on the training set, summarise the key features/observations in the data: focus on the dimensionality, data ranges, feature and class distribution and report anything out of the ordinary. What are the typical values of the features like?}{2}{subsection.1.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(3 points) Looking now at the Testing set, how does it compare with the Training Set (in terms of sizes and feature-distributions) and what could be the repurcussions of this?}{3}{subsection.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(2 points) Why do you think it is useful to consider TF-IDF weights as opposed to just the frequency of times a word appears in a document as a feature?}{4}{subsection.1.3}}
\newlabel{Q_UNSUP_LEARN}{{2}{5}{: \label {Q_UNSUP_LEARN}(24 points) Unsupervised Learning}{section.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}: (24 points) Unsupervised Learning}{5}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(2 points) The K-Means algorithm is non-deterministic. Explain why this is, and how the final model is selected in the SKLearn implementation of \href  {https://scikit-learn.org/stable/modules/clustering.html}{KMeans}.}{5}{subsection.2.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(1 point) One of the parameters we need to specify when using k-means is the number of clusters. What is a reasonable number for this problem and why?}{6}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(5 points) We will use the Adjusted Mutual Information (AMI) i.e.\spacefactor \@m {}\xspace  \href  {https://scikit-learn.org/stable/modules/clustering.html\#mutual-info-score}{\texttt  {adjusted\_mutual\\\_info\_score}} between the clusters and the true (known) labels to quantify the performance of the clustering. Give an expression for the MI in terms of entropy. In short, describe what the MI measures about two variables, why this is applicable here and why it might be difficult to use in practice. \emph  {Hint: MI is sometimes referred to as Information Gain: note that you are asked only about the standard way we defined MI and not the AMI which is adjusted for the size of the domain and for chance agreement.}}{7}{subsection.2.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)}(4 points) Fit K-Means objects with \texttt  {n\_clusters} ranging from 2 to 12. Set the random seed to 1000 and the number of initialisations to 50, but leave all other values at default. For each fit compute the adjusted mutual information (there is an SKLearn \href  {https://scikit-learn.org/stable/modules/generated/sklearn.metrics.adjusted_mutual_info_score.html}{function} for that). Set \texttt  {average\_method=`max'}. Plot the AMI scores against the number of clusters (as a line plot).}{8}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)}(3 points) Discuss any trends and interesting aspects which emerge from the plot. Does this follow from your expectations?}{9}{subsection.2.5}}
\newlabel{Q_CLUSTER_FOUR}{{(f)}{10}{\label {Q_CLUSTER_FOUR}(6 points) Let us investigate the case with four (4) clusters in some more detail. Using seaborn's \href {https://seaborn.pydata.org/generated/seaborn.countplot.html}{\texttt {countplot}} function, plot a bar-chart of the number of data-points with a particular class (encoded by colour) assigned to each cluster centre (encoded by position on the plot's x-axis). As part of the cluster labels, include the total number of data-points assigned to that cluster}{subsection.2.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(f)}(6 points) Let us investigate the case with four (4) clusters in some more detail. Using seaborn's \href  {https://seaborn.pydata.org/generated/seaborn.countplot.html}{\texttt  {countplot}} function, plot a bar-chart of the number of data-points with a particular class (encoded by colour) assigned to each cluster centre (encoded by position on the plot's x-axis). As part of the cluster labels, include the total number of data-points assigned to that cluster.}{10}{subsection.2.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(g)}(3 points) How does the clustering in Question\ref  {Q_UNSUP_LEARN}:\ref  {Q_CLUSTER_FOUR} align with the true class labels? Does it conform to your observations in Q 2(e)?}{11}{subsection.2.7}}
\@writefile{toc}{\contentsline {section}{\numberline {3}: (26 points) Logistic Regression Classification}{12}{section.3}}
\newlabel{Q_LR_NG}{{3}{12}{: (26 points) Logistic Regression Classification}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(3 points) What is the F1-score, and why is it preferable to accuracy in our problem? How does the macro-average work to extend the score to multi-class classification?}{12}{subsection.3.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(2 points) As always we start with a simple baseline classifier. Define such a classifier (indicating why you chose it) and report its performance on the \textbf  {Test} set. Use the `macro' average for the \texttt  {f1\_score}.}{13}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(3 points) We will now train a \href  {https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{LogisticRegression} Classifier from SKLearn. By referring to the documentation, explain how the Logistic Regression model can be applied to classify multi-class labels as in our case. \emph  {Hint: Limit your explanation to methods we discussed in the lectures.}}{14}{subsection.3.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)}(4 points) Train a Logistic Regressor on the training data. Set \texttt  {solver=`lbfgs'}, \texttt  {multi\_class=`multinomial'} and \texttt  {random\_state=0}. Use the Cross-Validation object you created and report the average validation-set F1-score as well as the standard deviation. Comment on the result.}{15}{subsection.3.4}}
\newlabel{Q_LOG_REG_PLT}{{(e)}{16}{\label {Q_LOG_REG_PLT}(5 points) We will now optimise the Regularisation parameter $C$ using cross-validation. Train a logistic regressor for different values of $C$: in each case, evaluate the F1 score on the training and validation portion of the fold. That is, for each value of $C$ you must provide the training set and validation-set scores per fold and then compute (and store) the average of both over all folds. Finally plot the (average) training and validation-set scores as a function of $C$. \hint {Use a logarithmic scale for $C$, spanning 19 samples between $10^{-4}$ to $10^5$.}}{subsection.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)}(5 points) We will now optimise the Regularisation parameter $C$ using cross-validation. Train a logistic regressor for different values of $C$: in each case, evaluate the F1 score on the training and validation portion of the fold. That is, for each value of $C$ you must provide the training set and validation-set scores per fold and then compute (and store) the average of both over all folds. Finally plot the (average) training and validation-set scores as a function of $C$. \emph  {Hint: Use a logarithmic scale for $C$, spanning 19 samples between $10^{-4}$ to $10^5$.}}{16}{subsection.3.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(f)}(7 points) What is the optimal value of $C$ (and the corresponding score)? How did you choose this value? By making reference to the effect of the regularisation parameter $C$ on the optimisation, explain what is happening in your plot from Question \ref  {Q_LR_NG}:\ref  {Q_LOG_REG_PLT} \emph  {Hint: Refer to the documentation for $C$ in the \href  {https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html}{LogisticRegression} page on SKLearn}.}{17}{subsection.3.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(g)}(2 points) Finally, report the score of the best model on the test-set, after retraining on the entire training set (that is drop the folds). \emph  {Hint: You may need to set \texttt  {max\_iter = 200}.} Comment briefly on the result.}{18}{subsection.3.7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}: (17 points) Hierarchical Classification}{19}{section.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(2 Marks) What aspects of the data may lend to better classification in such a hierarchical fashion? }{19}{subsection.4.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(3 points) First train a Logistic Regressor on the high-level classes (i.e.\spacefactor \@m {}\xspace  the four super-labels): use the same setup as before (Question \ref  {Q_LR_NG}), optimising the regularisation parameter $C$ over the folds. Report the best validation-set F1-score (average over folds) together with the optimal value of $C$. \emph  {Hint: Remember to keep track of the \textbf  {best} classifier trained on the \textbf  {entire} dataset (i.e.\spacefactor \@m {}\xspace  training data with no folds).}}{20}{subsection.4.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(2 points) We will now train individual binary classifiers for each of the groups. Why should we use the true super-class targets and not the ones predicted from the above classifier?}{21}{subsection.4.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)}(7 points) Train four independent Logistic Regression (binary) classifiers on the two classes within each super-group. That is, for each super-group, extract only the samples corresponding to that super-group using the true group label, then optimise a Logistic Regression classifier on the data with the targets being the two classes in that super-group. Report in each case the regularisation value which gives the best validation-set score as well as the associated F1-score. \emph  {Hint: This would look best in a table. Also, remember to keep track of the best classifier trained on the entire group in each case.}}{22}{subsection.4.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)}(3 points) Using the trained classifiers in a hierarchical fashion, evaluate the resulting model on the testing set for the full 8-way classification. \emph  {Hint: You will need to first generate the group-level predictions using the base classifier, and then, conditioned on this, predict the individual labels. Make sure to construct the indices correctly}. How does this compare to the original single-layer classifier?}{23}{subsection.4.5}}
\newlabel{Q_EXPLORATORY}{{5}{25}{: \label {Q_EXPLORATORY}(30 Points) Exploratory Analysis}{section.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}: (30 Points) Exploratory Analysis}{25}{section.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(6 points) Summarise the key features/observations in the data: describe the purpose of each column and report (briefly) also on the dimensionality/ranges (ballpark figures only, and how they compare across features) and number of sites, and identify anything out of the ordinary/problematic: i.e.\spacefactor \@m {}\xspace  look out for missing data and negative values. Why are the latter unreasonable in such a dataset? \emph  {Hint: Refer to the documentation for how to interpret the pollutant values.}}{25}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(6 points) Repeat the same analysis but this time on a per-site basis. Provide a table with the number of samples and percentage of problematic samples (negative and missing) in each site. To report numbers, count a row which has at least one missing entry as having missing data, and similarly for negative entries. \emph  {Hint: Pandas has a handy method, \texttt  {to\_latex()}, for generating a latex table from a dataframe.}}{26}{subsection.5.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(4 points) Briefly summarise how the sites compare in terms of number of samples and amount of problematic samples.}{27}{subsection.5.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)}(3 points) Given that the columns are all oxides of nitrogen and hence we expect them to be related, we will now look at correlations in our data. This will also be useful in determining how well we can predict any one of the readings from the other two. Remove the data from sites 3 and 15 and compute the \textbf  {Pearson} correlation coefficient between each of the three pollutant columns on the remaining data. Visualise the coefficients between each pair of columns in a table.}{28}{subsection.5.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)}(2 points) Comment on the level of correlation between each pair of pollutants.}{29}{subsection.5.5}}
\newlabel{CORRELATIONS}{{(f)}{30}{\label {CORRELATIONS}(5 points) For each of the three pollutants, compute the Pearson correlation between sites. \hint {You will need to remove the `Date Time' column and then group by the first level of the columns.} Then plot these as three heatmaps: show the values within the figures. \hint {Use the method \texttt {plot\_matrix()} from \texttt {mpctools.extensions.mplext}.}}{subsection.5.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(f)}(5 points) For each of the three pollutants, compute the Pearson correlation between sites. \emph  {Hint: You will need to remove the `Date Time' column and then group by the first level of the columns.} Then plot these as three heatmaps: show the values within the figures. \emph  {Hint: Use the method \texttt  {plot\_matrix()} from \texttt  {mpctools.extensions.mplext}.}}{30}{subsection.5.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(g)}(4 points) Comment briefly on your observations from Question \ref  {Q_EXPLORATORY}:\ref  {CORRELATIONS}: start by summarising the results from the NO gas and then comment on whether the same is observed in the other gases or if there is something different.}{31}{subsection.5.7}}
\@writefile{toc}{\contentsline {section}{\numberline {6}: (19 Points) Principal Component Analysis}{32}{section.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(1 point) Plot the first 5 lines of data (plot each row as a single line-plot).}{32}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(5 points) We will focus first on data solely from Site 1. Extract the data from this site, and run PCA with the number of components set to 72 for now. Set the \texttt  {random\_state=0}. On a single graph plot: (i) the percentage of the variance explained by each principal component (as a bar-chart), (ii) the cumulative variance (line-plot) explained by the first $n$ components: (\emph  {Hint: you should use \href  {https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.axes.Axes.twinx.html}{\texttt  {twinx()}} to make the plot fit}), \textsl  {and}, (iii) mark the point at which the number of components collectively explain at least 95\% of the variance (using a vertical line). \emph  {Hint: Number components starting from 1.}}{33}{subsection.6.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(2 points) Interpret and summarise the above plot.}{34}{subsection.6.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)}(5 points) Generate three figures, one for the mean and one for each of the first 2 principal components: in each, plot the mean/component as three lines, one for each pollutant throught one day cycle. \emph  {Hint: You will need to reshape the components appropriately.}}{35}{subsection.6.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)}(6 points) Focusing on the mean and first principal component, are there any significant patterns which emerge throughout the day? \emph  {Hint: Think about car usage throughout the day.} What is different when interpreting the mean versus the first component? \emph  {Hint: Do peaks signify the same thing in both cases?} Looking at the principal components only, are there any significant differences between the pollutants? Why could this be happening? \emph  {Hint: You can refer to one of the limitations of PCA.}}{36}{subsection.6.5}}
\newlabel{Q_LR_BA}{{7}{37}{: \label {Q_LR_BA}(49 points) Regression}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}: (49 points) Regression}{37}{section.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(a)}(2 points) First things first: since we are dealing with a supervised task, we will need to split our data into a training and testing set. Furthermore, since some of our regressors will involve hyper-parameter tuning, we will also need a validation set. Use the \texttt  {multi\_way\_split()} method from \texttt  {mpctools.extensions.skext} to split the data into a Training (60\%), Validation (15\%) and Testing (25\%) set: use the \href  {https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.ShuffleSplit.html}{ShuffleSplit} object from sklearn for the \texttt  {splitter}. Set the random state to 0. \emph  {Hint: The method gives you the indices of the split for each set, which can then be applied to multiple matrices.} Report the sizes of each dataset.}{37}{subsection.7.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(b)}(4 points) Let us start with a baseline. By using only the $y$-values, what baseline regressor can you define (indicate what it does)? Implement it and report the RMSE on the training and validation sets. Interpret this relative to the statistics of the data.}{38}{subsection.7.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(c)}(3 points) Let us now try a more interesting algorithm: specifically, we will start with \href  {https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}{LinearRegression}. Train the regressor on the training data and report the RMSE on the training and validation set, and comment on the relative performance to the baseline.}{39}{subsection.7.3}}
\newlabel{SQ_LR_RESID}{{(d)}{40}{\label {SQ_LR_RESID}(2 points) Another way of evaluating the applicability of a linear model is to analsye the residuals (errors). The Linear Regression model assumes a Gaussian form for the residuals. Fit a Gaussian to the errors (in the validation data) and report the mean and standard deviation}{subsection.7.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(d)}(2 points) Another way of evaluating the applicability of a linear model is to analsye the residuals (errors). The Linear Regression model assumes a Gaussian form for the residuals. Fit a Gaussian to the errors (in the validation data) and report the mean and standard deviation.}{40}{subsection.7.4}}
\newlabel{SQ_LR_RESID_PLT}{{(e)}{41}{\label {SQ_LR_RESID_PLT}(4 points) Plot a \href {https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html}{histogram} of both the residuals and the fitted Gaussian. The easiest way to generate a histogram of the Gaussian, is to generate a large number of samples ($\approx 10\times $ the amount of samples in the data) from the distribution (\hint {refer to \href {https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html}{norm.rvs} from \texttt {scipy.stats}}), and feed them to the hist method together with the residuals: \ie calling \texttt {plt.hist(x=[residuals, samples])}. Use 50 bins in the range [-250, 250] and visualise a density plot rather than raw counts}{subsection.7.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(e)}(4 points) Plot a \href  {https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.hist.html}{histogram} of both the residuals and the fitted Gaussian. The easiest way to generate a histogram of the Gaussian, is to generate a large number of samples ($\approx 10\times $ the amount of samples in the data) from the distribution (\emph  {Hint: refer to \href  {https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.norm.html}{norm.rvs} from \texttt  {scipy.stats}}), and feed them to the hist method together with the residuals: i.e.\spacefactor \@m {}\xspace  calling \texttt  {plt.hist(x=[residuals, samples])}. Use 50 bins in the range [-250, 250] and visualise a density plot rather than raw counts.}{41}{subsection.7.5}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(f)}(2 points) By referring to the plot in Question \ref  {Q_LR_BA}:\ref  {SQ_LR_RESID_PLT}, comment on whether your assumption in Question \ref  {Q_LR_BA}:\ref  {SQ_LR_RESID} is valid.}{42}{subsection.7.6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(g)}(5 points) We want to explore further what the model is learning. Explain why in Linear Regression, we cannot just blindly use the weights of the regression coefficients to evaluate the relative importance of each feature, but rather we have to normalise the features. By referring to the documentation for the \href  {http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html}{LinearRegression} implementation in SKLearn, explain what the normalisation does and how it helps in comparing features. Will this affect the performance of the Linear Regressor?}{43}{subsection.7.7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(h)}(5 points) Retrain the regressor, setting \texttt  {normalize=True} and report (in a table) the ratio of the relative importance of each feature. Which is the most/least important site? How do they compare with the correlation coefficients for Site 17 as computed in Question \ref  {Q_EXPLORATORY}:\ref  {CORRELATIONS}, and why do you think that is?}{44}{subsection.7.8}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(i)}(5 points) It might be that with non-linear models, we may get better performance. Let us try to use \href  {https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html}{K-Nearest-Neighbours}. Train a KNN regressor with default parameters on the training set and report performance on the training and validation set. \emph  {Hint: it might be beneficial to set \texttt  {n\_jobs=-1} to improve performance.} How does it compare with Linear Regression in terms of performance on both sets? What is a limitation of the KNN algorithm for our dataset?}{45}{subsection.7.9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(j)}(4 points) The KNN regression allows setting a number of hyper-parameters. We will optimise only one: the number of neighbours to use. By using the validation set, find the optimal value for the \texttt  {n\_neighbours} parameter out of the values [2, 4, 8, 16, 32]. Plot the training/validation RMSE and indicate (for example with a line) the best value for \texttt  {n\_neighbours}.}{46}{subsection.7.10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(k)}(1 points) What is the best-case RMSE performance on the validation set for KNN?}{47}{subsection.7.11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(l)}(4 points) Let us try one last regression algorithm: we will now use \href  {https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html}{DecisionTreeRegressor}. Again, the algorithm contains a number of hyper-parameters, and we will optimise the depth of the tree. Train a series of Decision Tree Regressors, optimising (over the validation set) the \texttt  {max\_depth} over the values [2, 4, 8, 16, 32, 64]. Set \texttt  {random\_state=0}. Plot the training/validation RMSE and indicate (as before) the best value for \texttt  {max\_depth}.}{48}{subsection.7.12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(m)}(3 points) What is the best-case RMSE performance on the validation set? What do you notice from the plot about the performance of the Decision Tree Regressor?}{49}{subsection.7.13}}
\@writefile{toc}{\contentsline {subsection}{\numberline {(n)}(5 points) To conclude let us now compare all the models on the testing set. Combine the training and validation sets and retrain the model from each family on it: in cases where we optimised hyper-parameters, set this to the best-case value. Report the testing-set performance of each model in a table \emph  {Hint: You should have 4 values}.}{50}{subsection.7.14}}
\newlabel{LastPage}{{}{50}{}{page.50}{}}
\xdef\lastpage@lastpage{50}
\xdef\lastpage@lastpageHy{50}
